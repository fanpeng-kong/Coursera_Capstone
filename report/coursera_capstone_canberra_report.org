#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:nil e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+title: The Battle of the Suburbs in Canberra
#+subtitle: Applied Data Science Capstone by IBM/Coursera
#+date: <2021-06-26 Sat>
#+author: Fanpeng Kong
#+email: fanpeng@fanpengkong.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.4.6)

#+latex_class: article
#+latex_class_options:
#+latex_header:
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_compiler: lualatex

#+PROPERTY: header-args :eval never-export

* Introduction: Business Problem
  :PROPERTIES:
  :CUSTOM_ID: introduction-business-problem-
  :END:

Canberra, the capital city of Australia, is often known as "The Bush
Capital" to Aussies (and yes, we put that on our vehicle number plates).
It is nothing like the usual metropolis like Madrid or Beijing which you
would expect from other countries. It is an entirely planned garden-like
city and in recent years, Canberra has been ranked among the world's
best cities to live. As per 2021, there are about 430 thousands
residents living accross the 814 square kilometeres region and the
poupulation density of Canberra is only about 528/km2. This figure is
not only low compared to other capital cities around the world but also
only ranks 6 among all the 8 Australia capitals.

However for any new settlers in Canberra, it could be a bit hard at the
beginning to find the right area to live in. Canberra has a total of more
than 100 local suburbs and 7 districts (or towncenters as Canberrans
refere to) and most of the facities, entertaiments and large shopping
centers are located in these towncenters. And if you ask any Canberran
for suggestions about the using the public transport system to travel
between these suburbs or towncenters, he/she propaly would recommend
you to get a car as the first thing after you find an accommadation. The
rental cost or property price among these suburbs can also differ to a
large extent based on their locations.

Assume that we have a young family who just moved from interstates or
overseas and would like to purchase a 3 or 4 bedrooms house in Canberra
to settle down. They set a budget for thier property hunting. What would
be the best suburbs that they should look into in terms of propety
price, travel distance to towncenters etc. In this study, I will try to
find the answer for them using the data science skills that I have
acquired during the courses.

* Data
  :PROPERTIES:
  :CUSTOM_ID: data-
  :END:

It is necessary to set out the required dataset for clustering and segmenting the suburbs first before attempting any data collection steps.
As the choice of the interesting suburb to purchase property is mainly based on several aspects such as real estate price and convenience to towncenters and access to local facilities.
After some considerations, I decided to use two different datasets for the suburbs clustering and segmentation analysis.
The main dataset includes the median house price with 3 or 4 bedrooms in each suburb, distance of each suburb to its nearest towncenters and the number of venues within 2km from each suburb's center point.
The second dataset mainly consists of types or category information of venues close to each suburb within the same distance.

During the data collection state, I exercised different methods from webscraping to open datasets which have been covered in previous courses:

- For the list of towncenters and suburbs in Canberra, webscraping was used to extract relevant fields from the table found in the Wikipedia page [[https://en.wikipedia.org/wiki/List_of_Canberra_suburbs][List of Canberra Suburbs]]

- Postcode for each suburb was extracted from an open dataset found on [[https://www.matthewproctor.com/australian_postcodes][Matthew Proctor's website]].

- The latitude and longitude of towncenters and suburbs were retrieved using the geocoder library, whereas the distance from suburb to towncenter was calculated using the [[https://developers.google.com/maps/documentation/distance-matrix][Google Distance Matrix API]] or the Python [[https://pypi.org/project/haversine/][Haversine]] library.

- The number and type of venues around each suburb was retrieved by calling the Forsquare APIs.

- The median prices for 3 or 4 bedrooms houses were scraped from [[https://www.domain.com.au/suburb-profile/][Domain's suburb profile]] page using =requests= and =BeautifulSoup= libraries.

In the following sections, steps to retrieve these data will be explained in more details.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
import numpy as np # library to handle data in a vectorized manner
import matplotlib.pyplot as plt

import pandas as pd # library for data analsysis
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import seaborn as sns

from scipy.spatial.distance import cdist

import json # library to handle JSON files

#!pip install geopy
from geopy.geocoders import Nominatim # convert an address into latitude and longitude values

import requests # library to handle requests
from bs4 import BeautifulSoup
from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe

# Matplotlib and associated plotting modules
import matplotlib.cm as cm
import matplotlib.colors as colors

# import k-means from clustering stage
from sklearn.cluster import KMeans

#!pip install folium=0.5.0|
import folium # map rendering library

print('Libraries imported.')
#+END_SRC

#+results:
: Libraries imported.

** Get towncenter and suburbs list
   :PROPERTIES:
   :CUSTOM_ID: get-towncenter-and-suburbs
   :END:

The first step for data collection was to get a list of towncenters and suburbs which belong to them. It turned out that this dataset with an easy to use format like .csv or spreadsheet is not easily found online.
The Wikipedia page of [[https://en.wikipedia.org/wiki/Suburbs_of_Canberra][Suburbs of Canberra]] includes such information in the expandable tables near the bottom.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
url = "https://en.wikipedia.org/wiki/Suburbs_of_Canberra"
data = requests.get(url).text
soup = BeautifulSoup(data, 'html.parser')
for table in soup.find_all('table'):
    print(table.get('class'))
#+END_SRC

#+results:
#+begin_example
['box-Unreferenced_section', 'plainlinks', 'metadata', 'ambox', 'ambox-content', 'ambox-Unreferenced']
['wikitable']
['nowraplinks', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'mw-collapsible', 'mw-collapsed', 'navbox-subgroup']
['nowraplinks', 'navbox-subgroup']
['nowraplinks', 'navbox-subgroup']
['nowraplinks', 'mw-collapsible', 'mw-collapsed', 'navbox-subgroup']
['nowraplinks', 'navbox-subgroup']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
['nowraplinks', 'hlist', 'mw-collapsible', 'autocollapse', 'navbox-inner']
#+end_example

A small challenge here was that those foldable tabels are nested and therefore same information may occure at different hierarchy levels.
After an cross examination between the webpage and the html inspector, I was able to narrow down the innerest talbe that containing the list of suburbs together with their corresponding towncenters.

[[file:figures/canberra_suburbs_wiki.png]]

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
tables = soup.find_all('table',  {'class': ['nowraplinks'] and ['navbox-subgroup']})
table_can = tables[1]
#+END_SRC

#+results:

I decided to separate the /Canberra Central/ district into /North Canberra/ and /South Canberra/ for the analysis.
The html source for each district row corresponds to a =tr= tag, within which are a =th= tag representing the /Towncenter/ name and a =td= tag consisting of an unordered list denoting the suburbs (=ul= tag).


[[file:figures/canberra_suburbs_html.png]]

# Let's define a function to retrieve the towncenter and suburbs from a
# given row:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# Get towncenter and surburbs from each table row
def get_suburbs(trow):
    towncenter = trow.find('th').text
    suburbs = []
    for l in trow.find('td').find_all('li'):
        suburbs.append(l.text)

    return towncenter, suburbs
#+END_SRC

#+results:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# createing the data frame
df = pd.DataFrame(columns=['Towncenter', 'Suburbs'])

for row in table_can.tbody.find_all('tr'):
    if row.find('th').text == 'Canberra Central':
        pass
    else:
        towncenter, suburbs = get_suburbs(row)
        df = df.append({'Towncenter': towncenter, 'Suburbs': suburbs}, ignore_index=True)

# df.head()
#+END_SRC

#+results:

After retrieved this table information, I separated each suburb to individual rows and create a Pandas DataFrame =df_can=.
This master DataFrame hosting the main datasets for the suburbs was updated in the following steps to include postcode, geo location, median house price, distance to towncenters and number of venues.
Hereafter I stored the =df_can= to individual .csv files at different stages which allowed  me to read the datasets from stored files instead of webscraping again for future runs should we wish (for speed or data consistency concerns).

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# convert suburb list elements to separate rows
# https://www.geeksforgeeks.org/convert-list-like-column-elements-to-separate-rows-in-pandas/
df_can = df['Suburbs'].apply(pd.Series) \
                    .merge(df, right_index=True, left_index=True) \
                    .drop(['Suburbs'], axis=1) \
                    .melt(id_vars=['Towncenter'], value_name='Suburb') \
                    .drop('variable', axis=1) \
                    .dropna()

df_can = df_can.sort_values('Suburb', ascending=True).reset_index(drop=True)
df_can.to_csv('data/canberra_suburbs.csv', index=False)
# df_can.head()
#+END_SRC

#+results:
** Add postcode
   :PROPERTIES:
   :CUSTOM_ID: add-postcode
   :END:

Although the postcode for each suburb was not directly used in this analysis, it was necessary to collect them as I needed them later when scraping the house price information from Domain website.
A FreeDatabase of Australian Postcodes have been found on [[https://www.matthewproctor.com/australian_postcodes][Matthew Proctor's website]].
After downloading the dataset, I filtered the postcode for Canberra suburbs and merged them into previous =df_can= DataFrame.
Apart from the postcode fields, this open dataset also includes other information such as geo location and zone data which might be useful for future study.
I decided to use =geopy= library to get the latitude and longitude by myself instead of using the data from this dataset as an exercise in next step.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
#df_can = pd.read_csv('data/canberra_suburbs.csv')
df_post = pd.read_csv('data/australian_postcodes.csv')
df_post_act = df_post[(df_post['state'] == 'ACT')][['postcode', 'locality']]
# df_post_act.head()
#+END_SRC

#+results:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_can = pd.merge(df_can, df_post_act, left_on=df_can['Suburb'].str.upper(), right_on=df_post_act['locality'].str.upper(), how='left').fillna(0)
df_can.drop(['key_0', 'locality'], axis=1, inplace=True)
df_can.rename(columns={'postcode': 'Postcode'}, inplace=True)
df_can['Postcode'] = df_can['Postcode'].astype(int)
df_can = df_can.drop_duplicates(subset=['Suburb'], keep='first')
df_can.to_csv('data/canberra_suburbs_postcode.csv', index=False)
# df_can.head()
#+END_SRC

#+results:

** Add geo location
   :PROPERTIES:
   :CUSTOM_ID: add-geo-location
   :END:

To get the latitude and longitude information for each suburb, the =geopy= library demonstrated in previous labs is used.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
from geopy.geocoders import Nominatim
#+END_SRC

#+results:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
#df_can = pd.read_csv('data/canberra_suburbs_postcode.csv')
geolocator = Nominatim(user_agent="act_agent")
print('Obtaining latitude and longitude: ', end='')
df_can['Latitude'] = 0.0
df_can['Longitude'] = 0.0
for index, row in df_can.iterrows():    
    address = '{}, ACT'.format(row['Suburb'])
    location = geolocator.geocode(address)
    df_can.loc[index, 'Latitude'] = location.latitude
    df_can.loc[index, 'Longitude'] = location.longitude
    print(' .', end='')

print(' done.')
df_can.to_csv('data/canberra_suburbs_geo.csv', index=False)
# df_can.head()
#+END_SRC

A separate DataFrame =df_town= was created to include the latitude and longitude information for towncenters.
After merging the geo location into main DataFrame =df_can=, it looks like below:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results :pandoc t
df_can = pd.read_csv('data/canberra_suburbs_geo.csv')
df_can.head()
#+END_SRC

#+results:
:RESULTS:
|   | Towncenter     | Suburb  | Postcode | Latitude   | Longitude  |
|---+----------------+---------+----------+------------+------------|
| 0 | North Canberra | Acton   | 2601     | -35.285232 | 149.112968 |
| 1 | North Canberra | Ainslie | 2602     | -35.262195 | 149.147880 |
| 2 | Gungahlin      | Amaroo  | 2914     | -35.169587 | 149.128021 |
| 3 | Belconnen      | Aranda  | 2614     | -35.258055 | 149.080426 |
| 4 | Tuggeranong    | Banks   | 2906     | -35.471889 | 149.099657 |
:END:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_can['Towncenter'].unique()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_town = pd.DataFrame(columns=['Towncenter', 'Latitude', 'Longitude'])

for towncenter in df_can['Towncenter'].unique():
    address = '{}, ACT'.format(towncenter)
    location = geolocator.geocode(address)
    df_town = df_town.append({'Towncenter': towncenter, 'Latitude': location.latitude, 'Longitude': location.longitude}, ignore_index=True)
    
df_town.to_csv('data/canberra_towncenters.csv', index=False)    
#df_town.head()
#+END_SRC

Based on the geo information of all the suburbs and towncenters, I created the following Folium map to visualize the towncenters and suburbs:

#+name: fig:map-towncenters
#+caption: Folium map for visualizing towncenters and suburbs
#+attr_latex: :float wrap :width 0.75\textwidth :center

[[file:figures/map_towncenters.jpg][file:figures/map_towncenters.jpg]]

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
address = 'Canberra, ACT'

geolocator = Nominatim(user_agent="act_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print('The geograpical coordinate of Canberra are {}, {}.'.format(latitude, longitude))
#+END_SRC

#+results:
: The geograpical coordinate of Canberra are -35.2975906, 149.1012676.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results
# create map of Toronto using latitude and longitude values
map_canberra = folium.Map(location=[latitude, longitude], zoom_start=10)

# add markers to suburbs
for lat, lng, towncenter, suburb in zip(df_can['Latitude'], df_can['Longitude'], df_can['Towncenter'], df_can['Suburb']):
    label = '{}, {}'.format(suburb, towncenter)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        color='blue',
        fill=True,
        fill_color='#3186cc',
        fill_opacity=0.7,
        parse_html=False).add_to(map_canberra)  
    
# add markers to towncenters
for lat, lng, towncenter in zip(df_town['Latitude'], df_town['Longitude'], df_town['Towncenter']):
    label = '{}'.format(towncenter)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=30,
        popup=label,
        color='purple',
        fill=True,
        fill_color='#EDE3FF',
        fill_opacity=0.3,
        parse_html=False).add_to(map_canberra)
    
map_canberra
#+END_SRC

#+results:

# The visulization looks all right so let's proceed to the next step to
# calulate the distance of each suburb to its nearest towncenter.

** Driving distance to nearest Towncenter
   :PROPERTIES:
   :CUSTOM_ID: driving-distance-to-nearest-towncenter
   :END:

My first attempt to calculate the driving distance between suburb to towncenter was to use [[https://developers.google.com/maps/documentation/distance-matrix/overview][Google Distance Matrix API]] following [[https://www.geeksforgeeks.org/python-calculate-distance-duration-two-places-using-google-distance-matrix-api/][this blog post]].
Unfortunately at the time of this study, there is no longer free options to use the Google Distance Matrix API.
As an alternative, I explored the [[https://towardsdatascience.com/calculating-distance-between-two-geolocations-in-python-26ad3afe287b][Haversine Distance]] as an alternative to the driving distance.

# First let's create a DataFrame holding the matrix of suburb (row) and towncenter (column):

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  import haversine as hs
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  to_columns = [town for town in df_town['Towncenter']]
  to_columns.append('Nearest')

  df_to = pd.DataFrame(columns=['Suburb']+to_columns)
  df_to['Suburb'] = df_can['Suburb']
  df_to.head()
#+END_SRC

By iterating each suburb and towncenter and calling the =haversine= function, a distance matrix representing the Haversine Distance between suburbs and towncenters were created and stored into a DataFrame.
An extra column =Nearest= hold the distance of a suburb to its closest towncenter was appended, although this nearest towncenter may not necessarily be the demographic one that a suburb belongs to.
This nearest distance to towncenter information was merged into the master DataFraem =df_can=.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  # Set towncenter and suburb as index for convenience
  df_to = df_to.set_index('Suburb')
  df_can = df_can.set_index('Suburb')
  df_town = df_town.set_index('Towncenter')
  for town in df_to.columns[:-1]:
      loc_town = (df_town.loc[town, 'Latitude'], df_town.loc[town, 'Longitude'])
      
      for sub in df_to.index.tolist():
          loc_sub = (df_can.loc[sub, 'Latitude'], df_can.loc[sub, 'Longitude'])
          dis = hs.haversine(loc_sub, loc_town)
          df_to.loc[sub, town] = dis
             
  # Nearest distance to towncenter
  df_to['Nearest'] = df_to[df_to.columns[:-1]].min(axis=1)
  # reset index
  df_to = df_to.reset_index()
  df_can = df_can.reset_index()
  df_town = df_town.reset_index()
  df_to.head()
#+END_SRC


#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  df_can = pd.merge(df_can, df_to[['Suburb', 'Nearest']], left_on='Suburb', right_on='Suburb', how='inner').fillna(0)
  df_can.to_csv('data/canberra_suburbs_distance.csv', index=False)
  df_can.head()
#+END_SRC

** Add median price
   :PROPERTIES:
   :CUSTOM_ID: add-median-price
   :END:

Again, the dataset for the real estate price for each suburb is not readily (or at least freely) available online.
Domain provides a [[https://www.domain.com.au/suburb-profile/][suburb profile page]] where you can enter the name of a suburb and serch its profile which happens to include a Market trends table.
For example, following figure shows the property sales information under a section named /Market trends/ [[https://www.domain.com.au/suburb-profile/banks-act-2906][found in suburb /Banks/]]:

[[file:figures/domain_price.png]]

The price of properties under interest are 3 or 4 bedrooms house for the young family who want to settle down in Canberra.
A little bit of exploration on the URLs for each suburb revealed that the suffix part has a pattern of =suburb-name/act/postcoe= with special character in the suburb name like single quote or space being replaced by a =-= character.
Using the webscraping techniques (=requests= and =BeautifulSoup=), I was able to extract the relevant field data from this table.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  def get_median_price(suburb, postcode):
      suburb = suburb.replace(' ', '-').replace("'", "-")
      url = 'https://www.domain.com.au/suburb-profile/{}-act-{}'.format(suburb.lower(), postcode)

      data = requests.get(url).text
      soup = BeautifulSoup(data, 'html.parser')
      table = soup.find('table',  {'class': ['css-15dn4s8']})

      df = pd.DataFrame(columns=['Bedrooms', 'Type', 'Median Price'])
      try:
          for body in table.find_all('tbody'):
              for row in body.find_all('tr'):
                  columns = row.find_all('td')

                  if(columns != []):
                      bedrooms = columns[0].text.strip()
                      type_ = columns[1].text.strip()
                      price = columns[2].text.strip().replace("$", "")
                      # convert price to numeric values
                      if price == '-':
                          price = None
                      else:
                          price = float(price[:-1]) * (10e3 if price[-1] == 'm' else 1) # convert to float

                      df = df.append({'Bedrooms':bedrooms, 'Type': type_, 'Median Price': price}, ignore_index=True)
      except:
          df = None

      return df
#+END_SRC

It is worth to note that not all suburbs have this Market trends table as it is based on the sales data in the past 12 months.
Additionally even the table exists, it is not always to have the price for 3/4 bedroom house for the very same reason.
I took care of these exceptions during the webscraping process and below I will explain how to handle the missing information for some suburbs.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  #df_can = pd.read_csv('data/canberra_suburbs_distance.csv')
  print('Obtaining house price: ', end='')
  df_price = pd.DataFrame(columns=['Suburb', 'bed3', 'bed4', 'Median Price'])
  df_price['Suburb'] = df_can['Suburb']

  for index, row in df_can.iterrows():
      suburb = row['Suburb']
      postcode = row['Postcode']
      df = get_median_price(suburb, postcode)
      
      if df is not None:
          df['Bedrooms'] = df['Bedrooms'].astype(int)
          # 3 bedrooms house price
          try:
              # not working
              p = df[(df['Type']=='House') & (df['Bedrooms']==3)]['Median Price'].values[0]
              df_price['bed3'][index] = p
          except:
              pass
          
          # 4 bedrooms house price
          try:
              p = df[(df['Type']=='House') & (df['Bedrooms']==4)]['Median Price'].values[0]
              df_price['bed4'][index] = p
          except:
              pass

      print(' .', end='')

  print(' done.')
  df_price.head()
#+END_SRC

*** Fixing missing prices
    :PROPERTIES:
    :CUSTOM_ID: fixing-missing-prices
    :END:

# Let's first check the number of suburbs that I didn't find a price for 3/4-bedroom houses as well as the number of thouse only has either 3 or
# 4-bedrooms:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
no_bed3_and_bed4 = df_price['bed3'].isnull() & df_price['bed4'].isnull()
no_bed3 = df_price['bed3'].isnull() & df_price['bed4'].notnull()
no_bed4 = df_price['bed3'].notnull() & df_price['bed4'].isnull()
total = df_price.shape[0]

print('No price for both 3 and 4 bedrooms: {}, missing {:.2f}%'.format(no_bed3_and_bed4.sum(), 
                                                                    no_bed3_and_bed4.sum()*100/total))
print('No price for 3 bedrooms: {}, missing {:.2f}%'.format(no_bed3.sum(),
                                                          no_bed3.sum()*100/total))
print('No price for 4 bedrooms: {}, missing {:.2f}%'.format(no_bed4.sum(),
                                                           no_bed4.sum()*100/total))
#+END_SRC

After a close examination of the price data for 3 or 4 bedrooms house, I found the missing rates for only 3 or 4 bedrooms are below 10% whereas missing rate for both 3 and 4 bedrooms are even higher 28%. Instead of dumping the rows with missing values or using the column mean to fill in the missing values directly which is likely to insert many same price values, I used the following strategy to fix the missing prices:

1. Assume the ratio for 4-bedroom and 3-bedroom price is similar in all the suburbs, calculate this ratio based on the average of suburbs which contain both values.
2. For the suburbs mising only one price, let's use the other price and the above average ratio to estimate the other missing price.
3. Finally for suburbs missing both prices, use the column mean respectively.

The average price for 3 and 4 bedrooms house was calculated and merged into the master DataFrame =df_can=.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
#1. calculate mean ration between 3 and 4 bedroom house price
df_p = df_price[df_price['bed3'].notnull() & df_price['bed4'].notnull()]
scale = (df_p['bed4'] / df_p['bed3']).mean()
#2. pupulating missing price using the mean scale based on the other existing price
df_price['bed4'][no_bed4] = df_price[no_bed4]['bed3'] * scale
df_price['bed3'][no_bed3] = df_price[no_bed3]['bed4'] / scale
#3. for those missing both 3&4 bedrooms, using the column mean
df_price['bed3'].fillna(value=df_price['bed3'].mean(), inplace=True)
df_price['bed4'].fillna(value=df_price['bed4'].mean(), inplace=True)
df_price.head()
#+END_SRC


#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
  df_price['Median Price'] = df_price[['bed3', 'bed4']].mean(axis=1)

  df_can = df_can.merge(df_price[['Suburb', 'Median Price']], left_on='Suburb', right_on='Suburb', how='inner')
  df_can.to_csv('data/canberra_suburbs_price.csv', index=False)
  df_can.head()
#+END_SRC

** Use Foursquare to check venues
   :PROPERTIES:
   :CUSTOM_ID: use-foursquare-to-check-venues
   :END:

# First define some credentials and a function to reteive venue information for a list of suburbs:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
CLIENT_ID = 'NA1JN3DQJULNX1LDJ00TD14RADCEPYZDJ1KSZEHK1LMVATL1' # your Foursquare ID
CLIENT_SECRET = 'RWZA2UXX3T1CENZTGVAWJ11FQRXSKO3B50NLOCAPLBIZ0NJY' # your Foursquare Secret
VERSION = '20180605' # Foursquare API version
LIMIT = 100 # A default Foursquare API limit value
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
def getNearbyVenues(names, latitudes, longitudes, radius=500):
    
    print('Obtaining venue information: ', end='')
    venues_list=[]
    for name, lat, lng in zip(names, latitudes, longitudes):            
        # create the API request URL
        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(
            CLIENT_ID, 
            CLIENT_SECRET, 
            VERSION, 
            lat, 
            lng, 
            radius, 
            LIMIT)
            
        # make the GET request
        results = requests.get(url).json()["response"]['groups'][0]['items']
        
        # return only relevant information for each nearby venue
        venues_list.append([(
            name, 
            lat, 
            lng, 
            v['venue']['name'], 
            v['venue']['location']['lat'], 
            v['venue']['location']['lng'],  
            v['venue']['categories'][0]['name']) for v in results])
        print(' .', end='')

    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])
    nearby_venues.columns = ['Suburb', 
                  'Suburb Latitude', 
                  'Suburb Longitude', 
                  'Venue', 
                  'Venue Latitude', 
                  'Venue Longitude', 
                  'Venue Category']
    
    print(' done.')
    return(nearby_venues)
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
#df_can = pd.read_csv('data/canberra_suburbs_price.csv')
canberra_venues = getNearbyVenues(names=df_can['Suburb'],
                                latitudes=df_can['Latitude'],
                                longitudes=df_can['Longitude'],
                                radius=2000
                                )
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
print('There are {} total categories.'.format(canberra_venues.shape[0]))
print('There are {} uniques categories.'.format(len(canberra_venues['Venue Category'].unique())))
canberra_venues.to_csv('data/canberra_venues.csv', index=False)
canberra_venues.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_count = canberra_venues.groupby(['Suburb'])['Suburb'].count().reset_index(name='venue count')
df_can = df_can.merge(df_count, left_on='Suburb', right_on='Suburb', how='left').fillna(0)
df_can.to_csv('data/canberra_suburbs_venues.csv', index=False)
df_can.head()
#+END_SRC

#+results:
:RESULTS:
|   | Suburb  | Towncenter     | Postcode | Latitude   | Longitude  | Nearest  | Median Price | venue count |
|---+---------+----------------+----------+------------+------------+----------+--------------+-------------|
| 0 | Acton   | North Canberra | 2601     | -35.285232 | 149.112968 | 2.921057 | 3044.733334  | 100.0       |
| 1 | Ainslie | North Canberra | 2602     | -35.262195 | 149.147880 | 1.413531 | 14295.000000 | 61.0        |
| 2 | Amaroo  | Gungahlin      | 2914     | -35.169587 | 149.128021 | 1.106799 | 726.000000   | 38.0        |
| 3 | Aranda  | Belconnen      | 2614     | -35.258055 | 149.080426 | 4.783948 | 3044.733334  | 29.0        |
| 4 | Banks   | Tuggeranong    | 2906     | -35.471889 | 149.099657 | 5.702031 | 655.000000   | 9.0         |
:END:

Similar to previous labs covered in the course, I used the Foursquare API to explore the venues around each suburb and categorize them.
As mentioned in the beginning, Canberra has a quite extended geography and a rather low population density, the radius to count the venues for each suburb was set to 2km.
The returned venues were grouped by each suburb and the total number of venues for each suburb was added into the main DataFrame.
This colcludes the end of main data collection and the master DataFrame =df_can= now looks like below:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results :pandoc t
df_can = pd.read_csv('data/canberra_suburbs_venues.csv')
df_can.head()
#+END_SRC

#+results:
:RESULTS:
|   | Suburb  | Towncenter     | Postcode | Latitude   | Longitude  | Nearest  | Median Price | venue count |
|---+---------+----------------+----------+------------+------------+----------+--------------+-------------|
| 0 | Acton   | North Canberra | 2601     | -35.285232 | 149.112968 | 2.921057 | 3044.733334  | 100.0       |
| 1 | Ainslie | North Canberra | 2602     | -35.262195 | 149.147880 | 1.413531 | 14295.000000 | 61.0        |
| 2 | Amaroo  | Gungahlin      | 2914     | -35.169587 | 149.128021 | 1.106799 | 726.000000   | 38.0        |
| 3 | Aranda  | Belconnen      | 2614     | -35.258055 | 149.080426 | 4.783948 | 3044.733334  | 29.0        |
| 4 | Banks   | Tuggeranong    | 2906     | -35.471889 | 149.099657 | 5.702031 | 655.000000   | 9.0         |
:END:

*** Venue category analysis

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
canberra_venues = pd.read_csv('data/canberra_venues.csv')

# one hot encoding
canberra_onehot = pd.get_dummies(canberra_venues[['Venue Category']], prefix="", prefix_sep="")

# add neighborhood column back to dataframe
canberra_onehot['Suburb'] = canberra_venues['Suburb'] 

# move neighborhood column to the first column
fixed_columns = [canberra_onehot.columns[-1]] + list(canberra_onehot.columns[:-1])
canberra_onehot = canberra_onehot[fixed_columns]

canberra_onehot.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
canberra_grouped = canberra_onehot.groupby('Suburb').mean().reset_index()
canberra_grouped.shape
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
def return_most_common_venues(row, num_top_venues):
    row_categories = row.iloc[1:]
    row_categories_sorted = row_categories.sort_values(ascending=False)
    
    return row_categories_sorted.index.values[0:num_top_venues]
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
num_top_venues = 10

indicators = ['st', 'nd', 'rd']

# create columns according to number of top venues
columns = ['Suburb']
for ind in np.arange(num_top_venues):
    try:
        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))
    except:
        columns.append('{}th Most Common Venue'.format(ind+1))

# create a new dataframe
df_venues = pd.DataFrame(columns=columns)
df_venues['Suburb'] = canberra_grouped['Suburb']

for ind in np.arange(canberra_grouped.shape[0]):
    df_venues.iloc[ind, 1:] = return_most_common_venues(canberra_grouped.iloc[ind, :], num_top_venues)

df_venues.to_csv('data/canberra_venues_sorted.csv', index=False)
df_venues.head()
#+END_SRC

After grouping the venues by each suburb, I sorted the type of venues for each suburb and found the top 10 most common venues in each suburb.
This dataset was added into an additional DataFrame =df_venues= and was used later on to cluster and segment the suburbs from the perspective of venues.
An example of the top 3 venues for each suburb is shown below and unsurprisingly Café and Hotels are found to be the most common ones in the suburbs close to CBD like Acton or Ainslie:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results :pandoc t
df_venues = pd.read_csv('data/canberra_venues_sorted.csv')
df_venues_sample = df_venues.drop(df_venues.columns[4:], axis=1)
df_venues_sample.head()
#+END_SRC

#+results:
:RESULTS:
|   | Suburb  | 1st Most Common Venue | 2nd Most Common Venue | 3rd Most Common Venue |
|---+---------+-----------------------+-----------------------+-----------------------|
| 0 | Acton   | Café                  | Hotel                 | Coffee Shop           |
| 1 | Ainslie | Café                  | Hotel                 | Chinese Restaurant    |
| 2 | Amaroo  | Café                  | Fast Food Restaurant  | Supermarket           |
| 3 | Aranda  | Gym                   | Café                  | Supermarket           |
| 4 | Banks   | Supermarket           | Sandwich Place        | Fried Chicken Joint   |
:END:

* Methodology
  :PROPERTIES:
  :CUSTOM_ID: methodology-
  :END:
In this study, I planned to categrize suburbs into different groups with similar profiles based on different metrics.
In the previous data collection stage, I have assembed two Pandas DataFrames: one consists of the median price for 3 to 4 bedroom houses, distance to closest towncenter and total number of venues in each suburb within 2km.
Whereas in the second DataFrame,the top 10 most common venues in each suburb are included.

In the next analysis step, I will first *check some statistics* of our dataset, for example, histogram of house prices and cloest distance to towncenter.
Then I perform unsupervised *k-means clustering* on these two datasets to cluster and segment suburbs.
After clustering, I will visiualize the clustered suburbs using Folium.

By the end of the study, I should have well segmented suburbs and be able to give recommendations to the young family seeking for properties according to different criteria.

* Analysis
  :PROPERTIES:
  :CUSTOM_ID: analysis-
  :END:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# uncomment if use stored data
df_can = pd.read_csv('data/canberra_suburbs_venues.csv')
df_town = pd.read_csv('data/canberra_towncenters.csv')
df_venues = pd.read_csv('data/canberra_venues_sorted.csv')

address = 'Canberra, ACT'
geolocator = Nominatim(user_agent="act_explorer")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
#+END_SRC

#+results:

** Get some statitics
   :PROPERTIES:
   :CUSTOM_ID: get-some-statitics
   :END:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# We will use the boxplot from Pandas as it provides groupby by default,
# but adjust the default style: https://stackoverflow.com/a/35197282

def adjust_boxplot_style(bp):
    # boxplot style adjustments
    [[item.set_linewidth(4) for item in bp[key]['boxes']] for key in bp.keys()]
    [[item.set_linewidth(4) for item in bp[key]['fliers']] for key in bp.keys()]
    [[item.set_linewidth(4) for item in bp[key]['medians']] for key in bp.keys()]
    [[item.set_linewidth(4) for item in bp[key]['means']] for key in bp.keys()]
    [[item.set_linewidth(4) for item in bp[key]['whiskers']] for key in bp.keys()]
    [[item.set_linewidth(4) for item in bp[key]['caps']] for key in bp.keys()]

    [[item.set_color('g') for item in bp[key]['boxes']] for key in bp.keys()]
    # seems to have no effect
    [[item.set_color('b') for item in bp[key]['fliers']] for key in bp.keys()]
    [[item.set_color('m') for item in bp[key]['medians']] for key in bp.keys()]
    [[item.set_markerfacecolor('k') for item in bp[key]['means']] for key in bp.keys()]
    [[item.set_color('c') for item in bp[key]['whiskers']] for key in bp.keys()]
    [[item.set_color('y') for item in bp[key]['caps']] for key in bp.keys()]
#+END_SRC

#+results:

Before proceeding to the clustering and segmentation on the suburbs, I performed some basic explanatory data analysis on the main DataFrame.
I explored statitics of median house price, number of venues and distance to nearest towncenters for each suburb grouped by their governing towncenters.

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results :file figures/suburb_statistics_towncenters.jpg
boxprops = dict(linestyle='-', linewidth=4, color='k')
medianprops = dict(linestyle='-', linewidth=4, color='k')

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))

bp1 = df_can.boxplot(column='Median Price',by='Towncenter', ax=ax1,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

bp2 = df_can.boxplot(column='venue count',by='Towncenter', ax=ax2,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

bp3 = df_can.boxplot(column='Nearest',by='Towncenter', ax=ax3,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

# boxplot style adjustments
adjust_boxplot_style(bp1)
adjust_boxplot_style(bp2)
adjust_boxplot_style(bp3)

# get rid of "boxplot grouped by" title
plt.suptitle("")

# label adjustment
ax1.set_title("Median price for suburbs \nin different towncenters", fontsize=20)
ax1.set_ylabel('Median Price (K)', fontsize=20)
ax1.set_xlabel('')
ax1.tick_params(axis='y', labelsize=20)
ax1.tick_params(axis='x', labelsize=20)
ax1.tick_params(axis="x", rotation=45)

ax2.set_title("Venue count for suburbs \nin different towncenters", fontsize=20)
ax2.set_ylabel('Venue numbers', fontsize=20)
ax2.set_xlabel('')
ax2.tick_params(axis='y', labelsize=20)
ax2.tick_params(axis='x', labelsize=20)
ax2.tick_params(axis="x", rotation=45)

ax3.set_title("Distance to nearest townceters for suburbs \nin different towncenters", fontsize=20)
ax3.set_ylabel('Distance (km)', fontsize=20)
ax3.set_xlabel('')
ax3.tick_params(axis='y', labelsize=20)
ax3.tick_params(axis='x', labelsize=20)
ax3.tick_params(axis="x", rotation=45)
#+END_SRC

#+results:
[[file:figures/suburb_statistics_towncenters.jpg]]

From the above 3 plots, it is evident that there are more venues in North and South Canberras and the house price there are also much higher than suburbs in other towncenters.
The distance for suburbs to closed towncenter however does not present a significant difference which indicate the well planned geo structure of Canberra suburbs.

** k-means clustering on master and venue category dataset 
   :PROPERTIES:
   :CUSTOM_ID: k-means-clustering-on-master-dataset
   :END:

K-means clustering is one of the most commonly used clustering techniques to perform unsupervised learning on datasets.
It aims to partition the dataset into /k/ clusters in which each sample belongs to the cluster with the nearest mean.
The k-means clustering minimises the data sample variance within clusters and therefor is useful for this study to group suburbs with similar profiles like median house price or distance to towncenters.

To perform k-means clustering on the master dataset =df_can=, several irrelevant columns such as /Towncenter/, /Postcode/, /Latitude/, /Longitude/ are dropped from the DataFrame.
Then the relevant data are normalized first before the clustering.
For k-means on the second venue category dataset, the number of different venue categories in each suburb are averaged first as a one-hot pre-processing.

To find the optimal number of clusters, the elbow method will be used where the optimal /k/ value is decided when the quality of the clustering see a sharp drop.
The tow figures below shows the distortion of the clusters on the two datasets and the optimal number of clusters are found to be 3 and 4 for the main and venue category datasets respectively. 

#+BEGIN_center
#+ATTR_LaTeX: :width 0.45\textwidth :center
[[file:figures/kmeans-optimal-master.pdf]]
#+ATTR_LaTeX: :width 0.45\textwidth :center
[[file:figures/kmeans-optimal-venue.pdf]]
#+END_center

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
#df_can = pd.read_csv('data/canberra_suburbs_venues.csv')
drop_columns = [j'Towncenter', 'Postcode', 'Latitude', 'Longitude']
df_cluster = df_can.drop(drop_columns, axis=1).set_index('Suburb')
#df_cluster.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# normalize datasets
from sklearn.preprocessing import StandardScaler
X = df_cluster.values[:,:]
X = np.nan_to_num(X)
clus_data = StandardScaler().fit_transform(X)
clus_data.shape
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none :results file
distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k, random_state=0).fit(clus_data)
    distortions.append(sum(np.min(cdist(clus_data, kmeanModel.cluster_centers_, 'canberra'), axis=1)) / clus_data.shape[0])

#There are different metric distance function for spatial distance. 
#I choose correlation instaed of euclidean because the canberra function gives me more clear view of elbow break point.

# Plot the elbow
fig, ax = plt.subplots(figsize=(6, 4))
ax.plot(K, distortions, 'bx-')
ax.set_xlabel('k')
ax.set_ylabel('Distortion')
ax.set_title('k-means on venue category dataset.')
plt.savefig('figures/kmeans-optimal-master.pdf')
#+END_SRC

#+results:
[[file:./.ob-jupyter/251b834fe4c1c24b5289c8f3a364f302b4b51685.png]]

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# set number of clusters
kclusters = 3
# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(clus_data)
kmeans.labels_
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# insert the cluster label to DataFrame
df_cluster = df_can.copy()
df_cluster.insert(0, 'Cluster', kmeans.labels_)
df_cluster.reset_index()
# df_cluster.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_cluster['Cluster'].value_counts()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
canberra_venues = pd.read_csv('data/canberra_venues.csv')
canberra_venues.head()

# one hot encoding
canberra_onehot = pd.get_dummies(canberra_venues[['Venue Category']], prefix="", prefix_sep="")

# add suburb column back to dataframe
canberra_onehot['Suburb'] = canberra_venues['Suburb'] 

# move Suburb column to the first column
list_column = canberra_onehot.columns.tolist()
number_column = int(list_column.index('Suburb'))
list_column = [list_column[number_column]] + list_column[:number_column] + list_column[number_column+1:] 
canberra_onehot = canberra_onehot[list_column]
#+END_SRC


#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
canberra_grouped = canberra_onehot.groupby('Suburb').mean().reset_index()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none :results file
canberra_grouped_clustering = canberra_grouped.drop('Suburb', 1)

distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k, random_state=0).fit(canberra_grouped_clustering)
    distortions.append(sum(np.min(cdist(canberra_grouped_clustering, kmeanModel.cluster_centers_, 'canberra'), axis=1)) / canberra_grouped_clustering.shape[0])
    
# Plot the elbow
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('k-means on venue category dataset.')
plt.savefig('figures/kmeans-optimal-venue.pdf')
plt.show()
#+END_SRC

#+results:
:RESULTS:
: /home/fanpeng/VirtualEnvs/Py3Station/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.
:   from ipykernel import kernelapp as app
[[file:./.ob-jupyter/3e97583325a9deb31f47f9d06e09b5c99d089762.png]]
:END:

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# set number of clusters
kclusters = 4

# run k-means clustering
kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(canberra_grouped_clustering)

# check cluster labels generated for each row in the dataframe
labels = kmeans.labels_
df_venues['Cluster'] = labels
df_venues.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
df_venues_merged = pd.merge(df_can, df_venues, left_on='Suburb', right_on='Suburb', how='inner')
df_venues_merged.head()
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(df_venues_merged['Latitude'], df_venues_merged['Longitude'], df_venues_merged['Suburb'], df_venues_merged['Cluster']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
    
# add markers to towncenters
for lat, lng, towncenter in zip(df_town['Latitude'], df_town['Longitude'], df_town['Towncenter']):
    label = '{}'.format(towncenter)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=30,
        popup=label,
        color='purple',
        fill=True,
        fill_color='#EDE3FF',
        fill_opacity=0.3,
        parse_html=False).add_to(map_clusters)
       
map_clusters
#+END_SRC

#+BEGIN_SRC jupyter-python :session py3 :async yes :exports none
# create map
map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)

# set color scheme for the clusters
x = np.arange(kclusters)
ys = [i + x + (i*x)**2 for i in range(kclusters)]
colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))
rainbow = [colors.rgb2hex(i) for i in colors_array]

# add markers to the map
markers_colors = []
for lat, lon, poi, cluster in zip(df_cluster['Latitude'], df_cluster['Longitude'], df_cluster['Suburb'], df_cluster['Cluster']):
    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)
    folium.CircleMarker(
        [lat, lon],
        radius=5,
        popup=label,
        color=rainbow[cluster-1],
        fill=True,
        fill_color=rainbow[cluster-1],
        fill_opacity=0.7).add_to(map_clusters)
    
# add markers to towncenters
for lat, lng, towncenter in zip(df_town['Latitude'], df_town['Longitude'], df_town['Towncenter']):
    label = '{}'.format(towncenter)
    label = folium.Popup(label, parse_html=True)
    folium.CircleMarker(
        [lat, lng],
        radius=30,
        popup=label,
        color='purple',
        fill=True,
        fill_color='#EDE3FF',
        fill_opacity=0.3,
        parse_html=False).add_to(map_clusters)
       
map_clusters
#+END_SRC

The two figures below showed Folium maps and the clustered suburbs based on the main dataset the venue category dataset, together with towncenters.
From the left figure, it is clear that most suburbs in North and South Canberra have been segmented into one category.
The other cluster include those suburbs which are very close to towncenters but do not belong to the inner north or south Canberra.
The rest suburbs which are located near the edge of each local towncenter make up the last category.

The right figure showing the suburbs clusters based on the venue category information tells a different story though.
While suburbs in North and South Canberra still fall in one category, they are joined by some suburbs from the Tuggeranong area.
The other obvious trend is that suburbs belonging to north and south towncenters seem to differ from the type of venues they host.
This might be related to the development and poupulation growth shift in Canberra happened in recent years but will need some further analysis.

#+BEGIN_center
#+ATTR_LaTeX: :width 0.45\textwidth :center
[[file:figures/map_cluster1.png]]
#+ATTR_LaTeX: :width 0.45\textwidth :center
[[file:figures/map_cluster2.png]]
#+END_center

** Revisit statistics on clustered suburbs
   :PROPERTIES:
   :CUSTOM_ID: revisit-statistics-on-clustered-suburbs
   :END:

After clustering and segmenting all the Canberra suburbs based on the main data set, i.e. median price for 3 or 4 bedrooms houses, distance to nearest towncenters and number of venues within 2km radius, the same statitics were performed again on the suburbs grouped by clusters instead of towncenters.
Cluster 2 includes most of the inner north and south suburbs in Canberra.
These suburbs cover the parliamentary triangle of Canberra and many of the national galleries and institutions can be found here too.
Therefore the number of venues are much higher than the other two clusters.
The downside of these suburbs are the much higher median house prices as shown in the first figure.
Cluster 1 suburbs mainly represent those close to local towncenters and in my opinion have the best trade-offs: surly the do not have as many venues as the inner ones but their house prices are also much lower.
The last cluster 0 is made up by the suburbs near the edge of each local towncenters evidenced by the nearest distance to towncenters shown in the right most figure.
They do not show many advantages compared to suburbs in the other two groups: for example, their distance to nearest towncenter almost doubled than those in cluster 1; the number of venues are much less while the median price for 3 or 4 bedrooms house showed a very similar profile to cluster 1 suburbs.


#+BEGIN_SRC jupyter-python :session py3 :async yes :exports results :file figures/suburbs_statistics_clusters.jpg
boxprops = dict(linestyle='-', linewidth=4, color='k')
medianprops = dict(linestyle='-', linewidth=4, color='k')

fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))

bp1 = df_cluster.boxplot(column='Median Price',by='Cluster', ax=ax1,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

bp2 = df_cluster.boxplot(column='venue count',by='Cluster', ax=ax2,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

bp3 = df_cluster.boxplot(column='Nearest',by='Cluster', ax=ax3,
                    showfliers=True, showmeans=True,
                    boxprops=boxprops, medianprops=medianprops,
                    return_type='dict')

# boxplot style adjustments
adjust_boxplot_style(bp1)
adjust_boxplot_style(bp2)
adjust_boxplot_style(bp3)

# get rid of "boxplot grouped by" title
plt.suptitle("")

# label adjustment
ax1.set_title("Median price for suburbs \nin different towncenters", fontsize=20)
ax1.set_ylabel('Median Price (K)', fontsize=20)
ax1.set_xlabel('')
ax1.tick_params(axis='y', labelsize=20)
ax1.tick_params(axis='x', labelsize=20)
ax1.tick_params(axis="x", rotation=45)

ax2.set_title("Venue count for suburbs \nin different towncenters", fontsize=20)
ax2.set_ylabel('Venue numbers', fontsize=20)
ax2.set_xlabel('')
ax2.tick_params(axis='y', labelsize=20)
ax2.tick_params(axis='x', labelsize=20)
ax2.tick_params(axis="x", rotation=45)

ax3.set_title("Distance to nearest townceters for suburbs \nin different towncenters", fontsize=20)
ax3.set_ylabel('Distance (km)', fontsize=20)
ax3.set_xlabel('')
ax3.tick_params(axis='y', labelsize=20)
ax3.tick_params(axis='x', labelsize=20)
ax3.tick_params(axis="x", rotation=45)
#+END_SRC

#+results:
[[file:figures/suburbs_statistics_clusters.jpg]]

* Results and Discussion
  :PROPERTIES:
  :CUSTOM_ID: results-and-discussion-
  :END:

Using the main dataset, most cluster 1 suburbs are located at the inner north or south Canberra.
While these suburbs are well established with various venues, their price are also much higher than other suburbs.
Suburbs in cluster 2 have the lowest price, but meanwhile they also do not have as many venues and the travel distance to towncenters are higher.
Cluster 0 suburbs seem to have the best balance for all three factors and not superisingly they are located around other towncenters apart from the North and South Canberra.

Clustering results using venue category information are even more interesting: suburbs in North and South Canberra (together with Tuggeranoon) again are clustered into one segment while suburbs in other north towns (Belconnen and Gungahlin) are segmented together.
Suburbs in Woden and Westen Creek share some similarities unlike other towns.
These difference might very well becasue of the developmnet shift from South Canberra to North Canberras. Due to the scope of the study, I will not furth explore the behind reasons.

Based on the above analysis, hopefully our young family now have more information to refere to when they finnaly decide which suburb to settle in.

* Conclusion
  :PROPERTIES:
  :CUSTOM_ID: conclusion-
  :END:

In this study, various techniques varing from webscraping to open dataset have been used to collect data related to Canberra suburbs.
Unsupervised k-means clustering algorithms have been applied to two assembed datasets: one master dataset consisting of median house price, number of venues and closest distance to nearby towncenters; while the second dataset includes venue category information around each suburb.

As per the decision to choose which suburb to live in, certainly more factors should be considered, e.g. local schools, developing trend. They are not included in this study due to the limited time and the already lengthy contet, but definitely will be helpful to provide an even accurate and detailed suburb segmentation.

Complete source of this capstone project [[https://github.com/fanpeng-kong/Coursera_Capstone][can be found on my Github]].
